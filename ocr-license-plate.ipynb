{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1156807,"sourceType":"datasetVersion","datasetId":654285},{"sourceId":1203932,"sourceType":"datasetVersion","datasetId":686454},{"sourceId":6210043,"sourceType":"datasetVersion","datasetId":3150001}],"dockerImageVersionId":30579,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport seaborn as sns\nimport torch\nimport cv2\nimport glob\nimport json\nimport matplotlib.pyplot as plt\nimport torch.nn as nn\nimport torch.nn.functional as functional\nimport matplotlib.patches as patches\nimport xml.etree.ElementTree as ET\nimport torchvision.transforms.functional as F\n\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torchvision import transforms\nfrom torchvision.transforms.functional import to_tensor\nfrom torchinfo import summary\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom torchinfo import summary\nfrom tensorflow.keras.backend import ctc_decode","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-05T12:02:32.418623Z","iopub.execute_input":"2023-12-05T12:02:32.418970Z","iopub.status.idle":"2023-12-05T12:02:47.718550Z","shell.execute_reply.started":"2023-12-05T12:02:32.418933Z","shell.execute_reply":"2023-12-05T12:02:47.717593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BASE_DIR          = '/kaggle/input/car-plates-ocr/data/'\nBASE_DIR_CPD      = '/kaggle/input/car-plate-detection/'\nBASE_DIR_CPD_ANNO = '/kaggle/input/car-plate-detection/annotations/'\nBASE_DIR_CPD_IMG  = '/kaggle/input/car-plate-detection/images/'\nIMAGE_SIZE        = (120, 50)\nIMAGE_SIZE_WHOLE  = (220, 220)\nDEVICE            = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nEPOCHS            = 15\nLEARNING_RATE     = 1e-3\nBATCH_SIZE        = 32\nOUTPUT            = 74 # Upper + lower case, numbers, unique symbols","metadata":{"execution":{"iopub.status.busy":"2023-12-05T12:02:47.720104Z","iopub.execute_input":"2023-12-05T12:02:47.720615Z","iopub.status.idle":"2023-12-05T12:02:47.725354Z","shell.execute_reply.started":"2023-12-05T12:02:47.720590Z","shell.execute_reply":"2023-12-05T12:02:47.724728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# car-plate-detection dataset","metadata":{}},{"cell_type":"code","source":"# data = {\n#     \"filename\": [],\n#     \"bbox\": []\n# }\n# for file in os.listdir(BASE_DIR_CPD_ANNO):\n#     tree = ET.parse(os.path.join(BASE_DIR_CPD_ANNO, file))\n#     root = tree.getroot()\n#     for obj in root.findall('object'):\n#         data['filename'].append(os.path.splitext(file)[0])\n        \n#         bndbox_elem = obj.find('bndbox')\n#         bbox = [int(bndbox_elem.find('xmin').text), int(bndbox_elem.find('ymin').text), int(bndbox_elem.find('xmax').text),  int(bndbox_elem.find('ymax').text)]\n#         data['bbox'].append(bbox)\n        \n# df = pd.DataFrame(data)\n# df[df['filename'].duplicated() == True]","metadata":{"execution":{"iopub.status.busy":"2023-12-05T12:02:47.726392Z","iopub.execute_input":"2023-12-05T12:02:47.726795Z","iopub.status.idle":"2023-12-05T12:02:47.857733Z","shell.execute_reply.started":"2023-12-05T12:02:47.726771Z","shell.execute_reply":"2023-12-05T12:02:47.856906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.imshow(BASE_DIR_CPD_IMG + \"Cars106.png\")","metadata":{"execution":{"iopub.status.busy":"2023-12-05T12:02:47.860040Z","iopub.execute_input":"2023-12-05T12:02:47.860596Z","iopub.status.idle":"2023-12-05T12:02:47.869084Z","shell.execute_reply.started":"2023-12-05T12:02:47.860564Z","shell.execute_reply":"2023-12-05T12:02:47.867987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# bboxes = []\n# for idx, col in df.iterrows():\n#     tree = ET.parse(BASE_DIR_CPD_ANNO + col['filename'] + \".xml\")\n#     root = tree.getroot()\n#     for obj in root.findall('object'):\n#         bndbox_elem = obj.find('bndbox')\n#         bbox = [int(bndbox_elem.find('xmin').text), int(bndbox_elem.find('ymin').text), int(bndbox_elem.find('xmax').text),  int(bndbox_elem.find('ymax').text)]\n#         bboxes.append(bbox)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T12:02:47.870528Z","iopub.execute_input":"2023-12-05T12:02:47.871679Z","iopub.status.idle":"2023-12-05T12:02:47.880829Z","shell.execute_reply.started":"2023-12-05T12:02:47.871636Z","shell.execute_reply":"2023-12-05T12:02:47.879984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# car-plates-ocr dataset","metadata":{}},{"cell_type":"code","source":"# Normalize the JSON data and create a DataFrame\ndf = pd.read_json('/kaggle/input/car-plates-ocr/data/train.json', orient='records')\n\n# Normalize the JSON data and create a DataFrame\ndf = pd.json_normalize(df.to_dict(orient='records'), 'nums', ['file'])","metadata":{"execution":{"iopub.status.busy":"2023-12-05T12:02:47.882194Z","iopub.execute_input":"2023-12-05T12:02:47.883244Z","iopub.status.idle":"2023-12-05T12:02:49.279511Z","shell.execute_reply.started":"2023-12-05T12:02:47.883195Z","shell.execute_reply":"2023-12-05T12:02:49.278608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_to_format(points):\n    x_0 = np.min([points[0][0], points[3][0]])\n    y_0 = np.min([points[0][1], points[1][1]])\n    x_1 = np.max([points[1][0], points[2][0]])\n    y_1 = np.max([points[2][1], points[3][1]])\n\n    if x_0 > x_1:\n        x_1, x_0 = x_0, x_1\n    if y_0 > y_1:\n        y_1, y_0 = y_0, y_1\n\n    return [x_0, y_0, x_1, y_1]\n\n# Apply the conversion function to the 'box' column\ndf['bbox'] = df['box'].apply(lambda x: convert_to_format(x))","metadata":{"execution":{"iopub.status.busy":"2023-12-05T12:02:49.280565Z","iopub.execute_input":"2023-12-05T12:02:49.280797Z","iopub.status.idle":"2023-12-05T12:02:49.918843Z","shell.execute_reply.started":"2023-12-05T12:02:49.280775Z","shell.execute_reply":"2023-12-05T12:02:49.917663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-05T12:02:49.919834Z","iopub.execute_input":"2023-12-05T12:02:49.920108Z","iopub.status.idle":"2023-12-05T12:02:49.941150Z","shell.execute_reply.started":"2023-12-05T12:02:49.920088Z","shell.execute_reply":"2023-12-05T12:02:49.940300Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gg = Image.open('/kaggle/input/car-plates-ocr/data/' + df.iloc[0]['file'])\nbounding_boxes = df.iloc[0]['bbox']\nbox = df.iloc[0]['box']\nfig, ax = plt.subplots(1)\n\n# Plot the image\nax.imshow(gg)\n\n# Plot bounding boxes\n# for bbox in bounding_boxes[:2]:\nx_0, y_0, x_1, y_1 = bounding_boxes\nwidth = x_1 - x_0\nheight = y_1 - y_0\nrect = patches.Rectangle((x_0, y_0), width, height, linewidth=1, edgecolor='r', facecolor='none')\nax.add_patch(rect)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T12:02:49.942207Z","iopub.execute_input":"2023-12-05T12:02:49.943087Z","iopub.status.idle":"2023-12-05T12:02:50.684914Z","shell.execute_reply.started":"2023-12-05T12:02:49.943056Z","shell.execute_reply":"2023-12-05T12:02:50.683674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_cars(images, labels, bboxes, count=10):\n    count = len(images)\n    rows = 2 # 2 rows\n    columns = (count // rows)\n    if count % 2 == 1:\n        columns += 1\n    \n    fig, ax = plt.subplots(rows, columns, figsize=(10,5))\n    ax = ax.flatten()\n    \n    for i, col in enumerate(images):\n        with Image.open(BASE_DIR + col) as car_image:\n            ax[i].imshow(car_image)\n            x_0, y_0, x_1, y_1 = bboxes[i]\n            width = x_1 - x_0\n            height = y_1 - y_0\n            rect = patches.Rectangle((x_0, y_0), width, height, linewidth=1, edgecolor='r', facecolor='none')\n            ax[i].set_xlabel(labels[i])\n            ax[i].set_xticks([])\n            ax[i].set_yticks([])\n            ax[i].add_patch(rect)\n    \n    # Remove unused\n    for j in range(count, rows * columns):\n        fig.delaxes(ax[j])\n        \n\nsamples = df.sample(10)\nshow_cars(samples['file'].to_list(), samples['text'].to_list(), samples['bbox'].to_list())","metadata":{"execution":{"iopub.status.busy":"2023-12-05T12:02:50.688436Z","iopub.execute_input":"2023-12-05T12:02:50.689255Z","iopub.status.idle":"2023-12-05T12:02:52.850877Z","shell.execute_reply.started":"2023-12-05T12:02:50.689221Z","shell.execute_reply":"2023-12-05T12:02:52.849688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def order_points(pts):\n    rect = np.zeros((4, 2), dtype = \"float32\")\n    \n    s = np.array(pts).sum(axis = 1)\n    rect[0] = pts[np.argmin(s)]\n    rect[2] = pts[np.argmax(s)]\n    \n    diff = np.diff(pts, axis = 1)\n    rect[1] = pts[np.argmin(diff)]\n    rect[3] = pts[np.argmax(diff)]\n    return rect\n\ndef four_point_transform(image, pts):\n    rect = order_points(pts)\n    \n    tl, tr, br, bl = pts\n    \n    width_1 = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2))\n    width_2 = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2))\n    max_width = max(int(width_1), int(width_2))\n    \n    height_1 = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2))\n    height_2 = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2))\n    max_height = max(int(height_1), int(height_2))\n    \n    dst = np.array([\n        [0, 0],\n        [max_width, 0],\n        [max_width, max_height],\n        [0, max_height]], dtype = \"float32\")\n    \n    M = cv2.getPerspectiveTransform(rect, dst)\n    warped = cv2.warpPerspective(np.array(image), M, (max_width, max_height))\n    return warped","metadata":{"execution":{"iopub.status.busy":"2023-12-05T12:02:52.852543Z","iopub.execute_input":"2023-12-05T12:02:52.853259Z","iopub.status.idle":"2023-12-05T12:02:52.865722Z","shell.execute_reply.started":"2023-12-05T12:02:52.853228Z","shell.execute_reply":"2023-12-05T12:02:52.864474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train, df_val = train_test_split(df, test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T12:02:52.867413Z","iopub.execute_input":"2023-12-05T12:02:52.867831Z","iopub.status.idle":"2023-12-05T12:02:52.889004Z","shell.execute_reply.started":"2023-12-05T12:02:52.867796Z","shell.execute_reply":"2023-12-05T12:02:52.887622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_characters = ''.join(df['text'].astype(str))\n\n# Get unique characters\nunique_characters = set(all_characters)\n\n# Print or use the unique characters as needed\nprint(len(unique_characters))","metadata":{"execution":{"iopub.status.busy":"2023-12-05T12:02:52.890722Z","iopub.execute_input":"2023-12-05T12:02:52.891623Z","iopub.status.idle":"2023-12-05T12:02:52.909054Z","shell.execute_reply.started":"2023-12-05T12:02:52.891584Z","shell.execute_reply":"2023-12-05T12:02:52.907220Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"characters = sorted(unique_characters)\nchar_to_index = {char: idx+1 for idx, char in enumerate(characters)}\nindex_to_char = {idx: char for char, idx in char_to_index.items()}\n\ncharacters = '0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'\nchar_to_index = {char: idx+1 for idx, char in enumerate(characters)}\nindex_to_char = {idx: char for char, idx in char_to_index.items()}\n\n\nto_remove = {\n    'train': [],\n    'val': []\n}\n\ndef label_to_indices(label, idx=None, is_train=True):\n    #         return [char_to_index[char] for char in label]\n    try:\n        res = []\n        for char in label:\n            if char not in char_to_index:\n                # Add the character to char_to_index and index_to_char\n                if idx != None:\n                    if is_train:\n                        to_remove['train'].append(idx)\n                    else:\n                        to_remove['val'].append(idx)\n                    break\n                    \n                index = len(char_to_index)\n                char_to_index[char] = index\n                index_to_char[index] = char\n            res.append(char_to_index[char])\n        return res\n    except KeyError as e:\n        print(f\"KeyError: {e} in label '{label}'\")\n        print([hex(ord(char)) for char in char_to_index])\n        print(str(e)[0])\n        print(char_to_index)\n        print(char_to_index.get(e))\n        raise\n    \ndef indices_to_label(indices):\n    return ''.join([index_to_char[idx] for idx in indices])\n\ndef generate_objectness_from_ground_truth(image_size, ground_truth_box):\n    \"\"\"\n    Generate objectness values based on a ground truth box.\n\n    Parameters:\n    - image_size (tuple): Size of the input image (height, width).\n    - ground_truth_box (torch.Tensor): Coordinates of the ground truth bounding box [x0, y0, x1, y1].\n\n    Returns:\n    - torch.Tensor: Objectness values (1 for positive region, 0 elsewhere).\n    \"\"\"\n    objectness = torch.zeros(image_size, dtype=torch.int)\n    # Extract coordinates of the ground truth box\n    x0, y0, x1, y1 = ground_truth_box\n    x0 = int(x0)\n    y0 = int(y0)\n    x1 = int(x1)\n    y1 = int(y1)\n    # Set the region around the ground truth box to 1 (positive region)\n    objectness[y0:y1, x0:x1] = 1\n\n    return objectness\n\nclass CarDataset(Dataset):\n    def __init__(self, df, image_size, transforms, for_detection=False):\n        super(CarDataset, self).__init__()\n        self.df = df\n        self.image_size = image_size\n        self.transform = transforms\n        self.index_to_char = {idx: char for char, idx in char_to_index.items()}\n        self.for_detection = for_detection\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        col = self.df.iloc[idx]\n        label = col['text']\n        box = col['box']\n        car_image = cv2.imread(BASE_DIR + col['file'])\n        car_image = cv2.cvtColor(car_image, cv2.COLOR_BGR2RGB)\n        \n        if self.for_detection:\n            new_width, new_height = IMAGE_SIZE_WHOLE\n            original_width, original_height, _ = car_image.shape\n            \n            car_image = cv2.resize(car_image, IMAGE_SIZE_WHOLE, interpolation = cv2.INTER_AREA)\n            \n            scale_width = new_width / original_width\n            scale_height = new_height / original_height\n            \n            x0, y0, x1, y1 = col['bbox']\n            scaled_x0 = x0 * scale_width\n            scaled_y0 = y0 * scale_height\n            scaled_x1 = x1 * scale_width\n            scaled_y1 = y1 * scale_height\n            \n            objectness = generate_objectness_from_ground_truth(IMAGE_SIZE_WHOLE, [scaled_x0, scaled_y0, scaled_x1, scaled_y1])\n            \n            car_image = self.transform(car_image)\n            return (car_image, [scaled_x0, scaled_y0, scaled_x1, scaled_y1], objectness)\n            \n        padded_image = car_image\n        # resize image\n        padded_image = four_point_transform(padded_image, box)\n        resized = cv2.resize(padded_image, dim, interpolation = cv2.INTER_AREA)\n        padded_image = self.transform(resized)\n        label_indices = label_to_indices(label)\n        return padded_image, label_indices\n    \n    def collate_fn(self, batch):\n        # Custom collate function to pad labels\n        images, labels = zip(*batch)\n\n        # Pad labels using pad_sequence\n        padded_labels = pad_sequence([torch.LongTensor(item) for item in labels], batch_first=True, padding_value=0)\n\n        # Convert padded_labels to a LongTensor\n        padded_labels = padded_labels.type(torch.LongTensor)\n\n        # Assuming images are already stacked into a tensor\n        images_tensor = torch.stack(images)\n\n        return images_tensor, padded_labels\n    \n    def custom_collate_detect_fn(self, batch):\n        car_images, bboxes, objectness = zip(*batch)\n        car_images = torch.stack(car_images)\n        objectness = torch.stack(objectness)\n        bboxes = torch.tensor(bboxes)\n        return car_images, bboxes, objectness","metadata":{"execution":{"iopub.status.busy":"2023-12-05T12:58:57.317870Z","iopub.execute_input":"2023-12-05T12:58:57.318246Z","iopub.status.idle":"2023-12-05T12:58:57.337748Z","shell.execute_reply.started":"2023-12-05T12:58:57.318215Z","shell.execute_reply":"2023-12-05T12:58:57.336768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-05T12:58:58.560520Z","iopub.execute_input":"2023-12-05T12:58:58.561540Z","iopub.status.idle":"2023-12-05T12:58:58.579246Z","shell.execute_reply.started":"2023-12-05T12:58:58.561476Z","shell.execute_reply":"2023-12-05T12:58:58.578097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Car Detection\n","metadata":{}},{"cell_type":"code","source":"transform_detect = transforms.Compose([\n    transforms.ToTensor(),\n#     transforms.RandomRotation(degrees=(-20, 20)),\n    transforms.GaussianBlur(kernel_size=3),\n# #     transforms.RandomAffine(degrees=5, shear=5, center=IMAGE_SIZE),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                         std=[0.229, 0.224, 0.225])\n])\ntrain_detect_ds = CarDataset(df_train, IMAGE_SIZE, transform_detect, True)\nval_detect_ds = CarDataset(df_val, IMAGE_SIZE, transform_detect, True)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T12:58:59.599123Z","iopub.execute_input":"2023-12-05T12:58:59.599480Z","iopub.status.idle":"2023-12-05T12:58:59.604882Z","shell.execute_reply.started":"2023-12-05T12:58:59.599452Z","shell.execute_reply":"2023-12-05T12:58:59.604222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_loader_detect = DataLoader(train_detect_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=train_detect_ds.custom_collate_detect_fn)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T12:59:00.231723Z","iopub.execute_input":"2023-12-05T12:59:00.232879Z","iopub.status.idle":"2023-12-05T12:59:00.237179Z","shell.execute_reply.started":"2023-12-05T12:59:00.232827Z","shell.execute_reply":"2023-12-05T12:59:00.236250Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CNN(nn.Module):\n    def __init__(self, num_anchors):\n        super(CNN, self).__init__()\n        \n        # CNN for feature extraction\n        self.cnn = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n            nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(1024),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n        )\n        \n        # Fully connected layer for classification\n        self.bbox_head = nn.Conv2d(1024, num_anchors * 4, kernel_size=3, padding=1)\n        # Objectness prediction head\n        self.objectness_head = nn.Sequential(\n            nn.Conv2d(1024, num_anchors * 1, kernel_size=3, padding=1),\n            nn.Conv2d(num_anchors * 1, 1, kernel_size=1)\n        )\n        \n\n    def forward(self, x):\n        # CNN\n        batch_size = x.size(0)\n        \n        x = self.cnn(x)\n        \n        bbox_predictions = self.bbox_head(x)\n\n        objectness_predictions = self.objectness_head(x)\n        return bbox_predictions, objectness_predictions","metadata":{"execution":{"iopub.status.busy":"2023-12-05T13:27:34.762124Z","iopub.execute_input":"2023-12-05T13:27:34.762502Z","iopub.status.idle":"2023-12-05T13:27:34.774763Z","shell.execute_reply.started":"2023-12-05T13:27:34.762475Z","shell.execute_reply":"2023-12-05T13:27:34.773604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_anchors = 5  # Number of anchor boxes per spatial position\nmodel_d = CNN(num_anchors)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T13:27:35.315883Z","iopub.execute_input":"2023-12-05T13:27:35.316286Z","iopub.status.idle":"2023-12-05T13:27:35.382877Z","shell.execute_reply.started":"2023-12-05T13:27:35.316238Z","shell.execute_reply":"2023-12-05T13:27:35.381339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary(model=model_d,\n        input_size=(1, 3, 220, 220),\n        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n        col_width=20,\n        row_settings=[\"var_names\"])","metadata":{"execution":{"iopub.status.busy":"2023-12-05T13:27:36.115600Z","iopub.execute_input":"2023-12-05T13:27:36.116006Z","iopub.status.idle":"2023-12-05T13:27:36.211140Z","shell.execute_reply.started":"2023-12-05T13:27:36.115979Z","shell.execute_reply":"2023-12-05T13:27:36.210046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_objectness(predicted_boxes, ground_truth_boxes, iou_threshold=0.5):\n    \"\"\"\n    Generate objectness values based on IoU between predicted boxes and ground truth boxes.\n\n    Parameters:\n    - predicted_boxes (torch.Tensor): Tensor of predicted bounding boxes in the format [x0, y0, x1, y1].\n    - ground_truth_boxes (torch.Tensor): Tensor of ground truth bounding boxes in the format [x0, y0, x1, y1].\n    - iou_threshold (float): IoU threshold to determine positive matches.\n\n    Returns:\n    - torch.Tensor: Objectness values (1 for positive matches, 0 for negative matches).\n    \"\"\"\n    batch_size = predicted_boxes.size(0)\n    objectness = torch.zeros(batch_size, dtype=torch.int)\n\n    for i in range(batch_size):\n        # Calculate IoU\n        iou = calculate_iou(predicted_boxes[i], ground_truth_boxes[i])\n\n        # Assign objectness value based on IoU threshold\n        objectness_value = 1 if iou > iou_threshold else 0\n        objectness[i] = objectness_value\n\n    return objectness\n\ndef calculate_iou(box1, box2):\n    \"\"\"\n    Calculate Intersection over Union (IoU) between two bounding boxes.\n\n    Parameters:\n    - box1 (torch.Tensor): Coordinates of the first bounding box [x0, y0, x1, y1].\n    - box2 (torch.Tensor): Coordinates of the second bounding box [x0, y0, x1, y1].\n\n    Returns:\n    - torch.Tensor: IoU value.\n    \"\"\"\n    intersection_x0 = torch.max(box1[0], box2[0])\n    intersection_y0 = torch.max(box1[1], box2[1])\n    intersection_x1 = torch.min(box1[2], box2[2])\n    intersection_y1 = torch.min(box1[3], box2[3])\n\n    intersection_area = torch.max(torch.zeros_like(intersection_x1 - intersection_x0),\n                                  (intersection_x1 - intersection_x0)) * torch.max(torch.zeros_like(intersection_y1 - intersection_y0),\n                                                                                 (intersection_y1 - intersection_y0))\n    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n\n    iou = intersection_area / (box1_area + box2_area - intersection_area + 1e-7)  # Adding a small epsilon to avoid division by zero\n\n    return iou","metadata":{"execution":{"iopub.status.busy":"2023-12-05T13:27:38.180473Z","iopub.execute_input":"2023-12-05T13:27:38.180843Z","iopub.status.idle":"2023-12-05T13:27:38.192152Z","shell.execute_reply.started":"2023-12-05T13:27:38.180814Z","shell.execute_reply":"2023-12-05T13:27:38.191020Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 10\ncriterion_bbox = nn.SmoothL1Loss()\ncriterion_objectness = nn.BCEWithLogitsLoss()\noptimizer_bbox = torch.optim.Adam(model_d.bbox_head.parameters(), lr=0.0001, weight_decay=1e-4)\noptimizer_objectness = torch.optim.Adam(model_d.objectness_head.parameters(), lr=0.0001, weight_decay=1e-4)\nnum_epochs = 20\npatience = 5\n\n# Training loop\nmodel.train()\ndef train_detect_model(model, train_loader, criterion_box, criterion_objectness, optimizer_box, optimizer_objectness, num_epochs=20):\n    model.train()\n    model.to(DEVICE)\n    best_loss = float('inf')\n    curr_patience = 0\n    \n    for epoch in range(num_epochs):\n        model.train()\n        loop = tqdm(train_loader, leave=True, desc=f'Epoch {epoch + 1}/{num_epochs}')\n\n        total_loss = 0\n        correct_predictions = 0\n        total_samples = 0\n\n        for batch_idx, (images, labels, objectness) in enumerate(loop):\n            images, labels, objectness = images.to(DEVICE), labels.to(DEVICE), objectness.to(DEVICE)\n            \n            batch_size = images.size(0)\n            \n            criterion_objectness.zero_grad()\n            optimizer_objectness.zero_grad()\n\n            # Forward pass\n            # outputs_box : self.bbox_head = nn.Conv2d(1024, num_anchors * 4, kernel_size=3, padding=1)\n            # outputs_objectness : self.objectness_head = nn.Conv2d(1024, num_anchors * 1, kernel_size=3, padding=1)\n            outputs_box, outputs_objectness = model(images)\n            \n            outputs_box = outputs_box.view(batch_size, -1, 4)\n            labels = labels.unsqueeze(1).expand_as(outputs_box)\n            \n#             outputs_objectness = outputs_objectness.view(batch_size, -1, num_anchors, 1)\n            \n            box_loss = criterion_box(outputs_box, labels)\n            objectness_loss = criterion_objectness(torch.sigmoid(outputs_objectness), objectness.unsqueeze(1).float())\n\n            # Backward pass\n            box_loss.backward()\n            objectness_loss_value.backward()\n\n            # Update weights\n            criterion_objectness.step()\n            optimizer_objectness.step()\n\n            # Update metrics\n            total_loss += loss.item()\n            \n#             _, predicted = torch.max(outputs, 2)\n#             correct_predictions += (predicted == labels).sum().item()\n#             total_samples += batch_size\n            \n            # Update tqdm bar description with the current loss and accuracy\n        \n            loop.set_postfix(loss_bbox=total_loss / (batch_idx + 1), loss_objectness=objectness_loss.item() / (batch_idx + 1))\n\n        epoch_loss = total_loss / len(train_loader)\n        # Save model checkpoint only if it's the best model so far\n        if epoch_loss < best_loss:\n            print(\"Saving\")\n            curr_patience = 0\n            best_loss = epoch_loss\n            checkpoint = {\n                'epoch': epoch + 1,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'loss': best_loss,\n            }\n            torch.save(checkpoint, 'detection_model.pth')\n        else:\n            curr_patience += 1\n        \n        if curr_patience > patience:\n            print('Stopping due to patience')\n            break\n            \n#         accuracy = correct_predictions / total_samples\n        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_loader)}')","metadata":{"execution":{"iopub.status.busy":"2023-12-05T13:28:19.322056Z","iopub.execute_input":"2023-12-05T13:28:19.322402Z","iopub.status.idle":"2023-12-05T13:28:19.336338Z","shell.execute_reply.started":"2023-12-05T13:28:19.322376Z","shell.execute_reply":"2023-12-05T13:28:19.335343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_detect_model(model_d, data_loader_detect, criterion_bbox, criterion_objectness, optimizer_bbox, optimizer_objectness)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T13:28:19.732165Z","iopub.execute_input":"2023-12-05T13:28:19.732517Z","iopub.status.idle":"2023-12-05T13:28:24.761893Z","shell.execute_reply.started":"2023-12-05T13:28:19.732491Z","shell.execute_reply":"2023-12-05T13:28:24.761031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## OCR Plate","metadata":{}},{"cell_type":"code","source":"for idx, col in df_train.iterrows():\n    label = col['text']\n    indexes = label_to_indices(label, idx)\n    \nfor idx, col in df_val.iterrows():\n    label = col['text']\n    indexes = label_to_indices(label, idx, False)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T06:02:17.180533Z","iopub.execute_input":"2023-12-05T06:02:17.181507Z","iopub.status.idle":"2023-12-05T06:02:19.529858Z","shell.execute_reply.started":"2023-12-05T06:02:17.181461Z","shell.execute_reply":"2023-12-05T06:02:19.528300Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.drop(index=to_remove['train'], inplace=True)\ndf_val.drop(index=to_remove['val'], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T06:01:02.406819Z","iopub.execute_input":"2023-12-05T06:01:02.407223Z","iopub.status.idle":"2023-12-05T06:01:02.425755Z","shell.execute_reply.started":"2023-12-05T06:01:02.407188Z","shell.execute_reply":"2023-12-05T06:01:02.424535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"DROP INDEX 26805 as it cause issue on converting it into image","metadata":{}},{"cell_type":"code","source":"df_train.drop(index=26805, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T06:01:02.465902Z","iopub.execute_input":"2023-12-05T06:01:02.466585Z","iopub.status.idle":"2023-12-05T06:01:02.482247Z","shell.execute_reply.started":"2023-12-05T06:01:02.466531Z","shell.execute_reply":"2023-12-05T06:01:02.480388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.ToTensor(),\n#     transforms.Resize(IMAGE_SIZE),\n#     transforms.RandomRotation(degrees=(-20, 20)),\n#     transforms.GaussianBlur(kernel_size=3),\n# #     transforms.RandomAffine(degrees=5, shear=5, center=IMAGE_SIZE),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                         std=[0.229, 0.224, 0.225])\n])\ntrain_ds = CarDataset(df_train, IMAGE_SIZE, transform)\nval_ds = CarDataset(df_val, IMAGE_SIZE, transform)","metadata":{"execution":{"iopub.status.busy":"2023-12-02T17:56:42.225982Z","iopub.execute_input":"2023-12-02T17:56:42.226348Z","iopub.status.idle":"2023-12-02T17:56:42.232072Z","shell.execute_reply.started":"2023-12-02T17:56:42.226317Z","shell.execute_reply":"2023-12-02T17:56:42.231071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# unique_characters = set()\n\n# for _, label in train_ds:\n#     unique_characters.update(set(label))\n\n# # Convert the set back to a list if needed\n# unique_characters_list = list(unique_characters)","metadata":{"execution":{"iopub.status.busy":"2023-12-02T17:56:43.641800Z","iopub.execute_input":"2023-12-02T17:56:43.642667Z","iopub.status.idle":"2023-12-02T17:56:43.646382Z","shell.execute_reply.started":"2023-12-02T17:56:43.642631Z","shell.execute_reply":"2023-12-02T17:56:43.645475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img, label = train_ds[2]\nplt.imshow(img.permute(1, 2, 0).numpy())\nplt.xlabel(indices_to_label(label))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-02T17:56:44.056101Z","iopub.execute_input":"2023-12-02T17:56:44.056822Z","iopub.status.idle":"2023-12-02T17:56:44.411890Z","shell.execute_reply.started":"2023-12-02T17:56:44.056788Z","shell.execute_reply":"2023-12-02T17:56:44.410936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=train_ds.collate_fn)","metadata":{"execution":{"iopub.status.busy":"2023-12-02T17:56:48.969077Z","iopub.execute_input":"2023-12-02T17:56:48.969708Z","iopub.status.idle":"2023-12-02T17:56:48.974638Z","shell.execute_reply.started":"2023-12-02T17:56:48.969675Z","shell.execute_reply":"2023-12-02T17:56:48.973639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gg = next(iter(data_loader))","metadata":{"execution":{"iopub.status.busy":"2023-12-02T17:56:51.591892Z","iopub.execute_input":"2023-12-02T17:56:51.592605Z","iopub.status.idle":"2023-12-02T17:56:52.652332Z","shell.execute_reply.started":"2023-12-02T17:56:51.592567Z","shell.execute_reply":"2023-12-02T17:56:52.651347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gg[1].shape","metadata":{"execution":{"iopub.status.busy":"2023-12-02T17:56:52.654287Z","iopub.execute_input":"2023-12-02T17:56:52.654933Z","iopub.status.idle":"2023-12-02T17:56:52.661173Z","shell.execute_reply.started":"2023-12-02T17:56:52.654871Z","shell.execute_reply":"2023-12-02T17:56:52.660222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MODEL","metadata":{}},{"cell_type":"code","source":"len(char_to_index)","metadata":{"execution":{"iopub.status.busy":"2023-12-02T17:56:54.347393Z","iopub.execute_input":"2023-12-02T17:56:54.347756Z","iopub.status.idle":"2023-12-02T17:56:54.354069Z","shell.execute_reply.started":"2023-12-02T17:56:54.347723Z","shell.execute_reply":"2023-12-02T17:56:54.353044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(index_to_char)","metadata":{"execution":{"iopub.status.busy":"2023-12-02T17:56:55.075210Z","iopub.execute_input":"2023-12-02T17:56:55.075854Z","iopub.status.idle":"2023-12-02T17:56:55.081531Z","shell.execute_reply.started":"2023-12-02T17:56:55.075817Z","shell.execute_reply":"2023-12-02T17:56:55.080604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CRNN(nn.Module):\n    def __init__(self, num_classes, cnn_out_channels=512, rnn_hidden_size=256):\n        super(CRNN, self).__init__()\n        \n        # CNN for feature extraction\n        self.cnn = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n#             nn.MaxPool2d(kernel_size=(2, 1), stride=(2, 1)),\n            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n#             nn.MaxPool2d(kernel_size=(2, 1), stride=(2, 1)),\n            \n            nn.Conv2d(512, cnn_out_channels, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n#             nn.MaxPool2d(kernel_size=(2, 1), stride=(2, 1))\n        )\n        \n        # RNN for sequence processing\n        self.rnn = nn.LSTM(6144, 512, bidirectional=True, batch_first=True, num_layers=1)\n#         self.rnn2 = nn.LSTM(1024, 512, bidirectional=True, batch_first=True, num_layers=1)\n        \n        # Fully connected layer for classification\n        self.fc1 = nn.Linear(512 * 2, 1024)\n        self.fc2 = nn.Linear(1024, num_classes)\n        self.dropout = nn.Dropout(0.2)\n        \n    def forward(self, x):\n        # CNN\n        x = self.cnn(x)\n        \n        # Reshape for RNN\n        batch_size, channels, height, width = x.size()\n        x = x.permute(0, 3, 1, 2).contiguous()\n        x = x.view(batch_size, width, channels * height)\n        # RNN\n        x, _ = self.rnn(x)\n#         x, _ = self.rnn2(x)\n        \n        # Fully connected layer\n#         x = self.fc(x)\n        x = self.fc1(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2023-12-02T17:57:08.119309Z","iopub.execute_input":"2023-12-02T17:57:08.119674Z","iopub.status.idle":"2023-12-02T17:57:08.132103Z","shell.execute_reply.started":"2023-12-02T17:57:08.119644Z","shell.execute_reply":"2023-12-02T17:57:08.130950Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = CRNN(len(char_to_index) + 1)","metadata":{"execution":{"iopub.status.busy":"2023-12-02T17:57:08.584611Z","iopub.execute_input":"2023-12-02T17:57:08.585019Z","iopub.status.idle":"2023-12-02T17:57:08.861003Z","shell.execute_reply.started":"2023-12-02T17:57:08.584984Z","shell.execute_reply":"2023-12-02T17:57:08.860157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary(model=model,\n        input_size=(1, 3, 50, 120),\n        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n        col_width=20,\n        row_settings=[\"var_names\"])","metadata":{"execution":{"iopub.status.busy":"2023-12-02T17:56:57.390248Z","iopub.execute_input":"2023-12-02T17:56:57.390615Z","iopub.status.idle":"2023-12-02T17:57:07.095389Z","shell.execute_reply.started":"2023-12-02T17:56:57.390583Z","shell.execute_reply":"2023-12-02T17:57:07.094479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint_path = \"crnn_checkpoint.pth\"","metadata":{"execution":{"iopub.status.busy":"2023-12-02T17:57:10.648627Z","iopub.execute_input":"2023-12-02T17:57:10.649498Z","iopub.status.idle":"2023-12-02T17:57:10.653373Z","shell.execute_reply.started":"2023-12-02T17:57:10.649458Z","shell.execute_reply":"2023-12-02T17:57:10.652474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 10\ncriterion = nn.CTCLoss(blank=0, zero_infinity=False)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\nnum_epochs = 20\npatience = 5\n\n# Training loop\nmodel.train()\ndef train_model(model, train_loader, criterion, optimizer, num_epochs=20):\n    model.train()\n    model.to(DEVICE)\n    best_loss = float('inf')\n    curr_patience = 0\n    \n    for epoch in range(num_epochs):\n        model.train()\n        loop = tqdm(train_loader, leave=True, desc=f'Epoch {epoch + 1}/{num_epochs}')\n\n        total_loss = 0\n        correct_predictions = 0\n        total_samples = 0\n\n        for batch_idx, (images, labels) in enumerate(loop):\n            images, labels = images.to(DEVICE), labels.to(DEVICE)\n            \n            batch_size = images.size(0)\n            \n            optimizer.zero_grad()\n\n            # Forward pass\n            outputs = model(images)  # outputs is torch.Size([N, 62, 51]])\n            log_probs = outputs  # (T, N, C) based on documentation\n            log_probs = nn.functional.log_softmax(log_probs, dim=2).permute(1, 0, 2)\n            input_lengths = torch.tensor([len(seq) for seq in outputs])\n    \n            target_lengths = torch.sum(labels != 0, dim=1)\n            target = labels.long()\n            \n            loss = criterion(log_probs, target, input_lengths, target_lengths)\n            \n            # Backward pass\n            loss.backward()\n\n            # Update weights\n            optimizer.step()\n\n            # Update metrics\n            total_loss += loss.item()\n            \n#             _, predicted = torch.max(outputs, 2)\n#             correct_predictions += (predicted == labels).sum().item()\n#             total_samples += batch_size\n            \n            # Update tqdm bar description with the current loss and accuracy\n        \n            loop.set_postfix(loss=total_loss / (batch_idx + 1))\n\n        epoch_loss = total_loss / len(train_loader)\n        # Save model checkpoint only if it's the best model so far\n        if epoch_loss < best_loss:\n            print(\"Saving\")\n            curr_patience = 0\n            best_loss = epoch_loss\n            checkpoint = {\n                'epoch': epoch + 1,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'loss': best_loss,\n            }\n            torch.save(checkpoint, checkpoint_path)\n        else:\n            curr_patience += 1\n        \n        if curr_patience > patience:\n            print('Stopping due to patience')\n            break\n            \n#         accuracy = correct_predictions / total_samples\n        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_loader)}')","metadata":{"execution":{"iopub.status.busy":"2023-12-02T17:57:55.847749Z","iopub.execute_input":"2023-12-02T17:57:55.848172Z","iopub.status.idle":"2023-12-02T17:57:55.863520Z","shell.execute_reply.started":"2023-12-02T17:57:55.848139Z","shell.execute_reply":"2023-12-02T17:57:55.862405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_model(model, data_loader, criterion, optimizer)","metadata":{"execution":{"iopub.status.busy":"2023-12-02T17:57:56.455536Z","iopub.execute_input":"2023-12-02T17:57:56.456361Z","iopub.status.idle":"2023-12-02T18:49:30.206066Z","shell.execute_reply.started":"2023-12-02T17:57:56.456319Z","shell.execute_reply":"2023-12-02T18:49:30.204757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Checkpoint","metadata":{}},{"cell_type":"code","source":"import keras","metadata":{"execution":{"iopub.status.busy":"2023-12-02T18:49:34.244074Z","iopub.execute_input":"2023-12-02T18:49:34.244562Z","iopub.status.idle":"2023-12-02T18:49:34.249483Z","shell.execute_reply.started":"2023-12-02T18:49:34.244521Z","shell.execute_reply":"2023-12-02T18:49:34.248399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint = torch.load(checkpoint_path)\nmodel.load_state_dict(checkpoint['model_state_dict'])","metadata":{"execution":{"iopub.status.busy":"2023-12-02T17:09:19.896250Z","iopub.execute_input":"2023-12-02T17:09:19.896615Z","iopub.status.idle":"2023-12-02T17:09:19.933443Z","shell.execute_reply.started":"2023-12-02T17:09:19.896585Z","shell.execute_reply":"2023-12-02T17:09:19.932518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img1, lbl1 = val_ds[54]\nimg1 = img1.to(DEVICE)","metadata":{"execution":{"iopub.status.busy":"2023-12-02T18:52:01.396338Z","iopub.execute_input":"2023-12-02T18:52:01.397054Z","iopub.status.idle":"2023-12-02T18:52:01.437881Z","shell.execute_reply.started":"2023-12-02T18:52:01.397019Z","shell.execute_reply":"2023-12-02T18:52:01.436938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\nwith torch.no_grad():\n    outputt = model(img1.unsqueeze(0)).cpu()\n    print(outputt.argmax(2))\n    text_out = ''.join([index_to_char[x] for x in outputt.argmax(2)[0].numpy() if x != 0 ] )\n\ninput_len = torch.tensor([len(seq) for seq in outputt])\npreds = keras.backend.ctc_decode(outputt, input_length=input_len, greedy=True)\ndecoded = ''.join([index_to_char[x] for x in preds[0][0][0].numpy() if x != 0 and x != -1])\n\nprint(\"Actual: \", indices_to_label(lbl1))\nprint(\"Predicted: \", text_out)\nprint(\"Predicted ctc decode: \", decoded)","metadata":{"execution":{"iopub.status.busy":"2023-12-02T18:52:01.615851Z","iopub.execute_input":"2023-12-02T18:52:01.616701Z","iopub.status.idle":"2023-12-02T18:52:01.635402Z","shell.execute_reply.started":"2023-12-02T18:52:01.616666Z","shell.execute_reply":"2023-12-02T18:52:01.634459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Extract Data from car-plate-detection dataset","metadata":{}},{"cell_type":"code","source":"def parse_xml(xml_path):\n    tree = ET.parse(xml_path)\n    root = tree.getroot()\n\n    filename = root.find('filename').text\n\n    # Extract bounding box information\n    bboxes = []\n    for obj in root.findall('object'):\n        xmin = int(obj.find('bndbox/xmin').text)\n        ymin = int(obj.find('bndbox/ymin').text)\n        xmax = int(obj.find('bndbox/xmax').text)\n        ymax = int(obj.find('bndbox/ymax').text)\n\n        bbox = {'xmin': xmin, 'ymin': ymin, 'xmax': xmax, 'ymax': ymax}\n        bboxes.append(bbox)\n\n    return filename, bboxes\ndata = {'file_name': [], 'bbox': []}\nfor file in os.listdir('/kaggle/input/car-plate-detection/annotations/'):\n    filename, bboxes = parse_xml(file)\n\n    for bbox in bboxes:\n        data['file_name'].append(filename)\n        data['bbox'].append([bbox['xmin'], bbox['ymin'], bbox['xmax'], bbox['ymax']])\n#         data['xmin'].append(bbox['xmin'])\n#         data['ymin'].append(bbox['ymin'])\n#         data['xmax'].append(bbox['xmax'])\n#         data['ymax'].append(bbox['ymax'])\n\n# Will be used for testing as this is just for detection not reading\ndf_test = pd.DataFrame(data) # Pascal voc format for bbox (Xmin, Ymin, Xmax, Ymax)","metadata":{"execution":{"iopub.status.busy":"2023-11-18T00:14:35.575096Z","iopub.status.idle":"2023-11-18T00:14:35.575693Z","shell.execute_reply.started":"2023-11-18T00:14:35.575397Z","shell.execute_reply":"2023-11-18T00:14:35.575424Z"},"trusted":true},"execution_count":null,"outputs":[]}]}